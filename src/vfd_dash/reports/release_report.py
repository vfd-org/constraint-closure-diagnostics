"""
Release Report Generator.

Generates markdown reports for individual runs, summarizing
configuration, results, figures, and reproducibility information.
"""

import json
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional, List


# Safety disclaimer snippet (from SAFETY_BOUNDARY.md)
SAFETY_DISCLAIMER = """
## Important Notice

This report is generated by a **diagnostic and visualization framework** for
exploring constraint-based approaches to the Riemann Hypothesis. It does not
claim, demonstrate, or provide a proof of RH.

- The "VFD invariants" are properties of a specific mathematical construction.
- The "Bridge Axiom" is an unverified hypothesis, not a proven theorem.
- Passing all internal checks does NOT imply RH is true.
"""


def get_git_info() -> Dict[str, str]:
    """Get git repository information for reproducibility."""
    info = {
        "git_hash": "unknown",
        "git_branch": "unknown",
        "git_dirty": "unknown",
    }

    try:
        # Get current commit hash
        result = subprocess.run(
            ["git", "rev-parse", "HEAD"],
            capture_output=True, text=True, timeout=5
        )
        if result.returncode == 0:
            info["git_hash"] = result.stdout.strip()[:12]

        # Get current branch
        result = subprocess.run(
            ["git", "rev-parse", "--abbrev-ref", "HEAD"],
            capture_output=True, text=True, timeout=5
        )
        if result.returncode == 0:
            info["git_branch"] = result.stdout.strip()

        # Check for uncommitted changes
        result = subprocess.run(
            ["git", "status", "--porcelain"],
            capture_output=True, text=True, timeout=5
        )
        if result.returncode == 0:
            info["git_dirty"] = "yes" if result.stdout.strip() else "no"

    except Exception:
        pass

    return info


def get_python_info() -> Dict[str, str]:
    """Get Python environment information."""
    import sys
    import platform

    info = {
        "python_version": sys.version.split()[0],
        "platform": platform.platform(),
    }

    # Try to get key package versions
    try:
        import numpy
        info["numpy_version"] = numpy.__version__
    except ImportError:
        pass

    try:
        import scipy
        info["scipy_version"] = scipy.__version__
    except ImportError:
        pass

    return info


def generate_release_report(
    run_dir: Path,
    config: Dict[str, Any],
    manifest: Dict[str, Any],
    metrics: Optional[Dict[str, Any]] = None,
    perf: Optional[Dict[str, Any]] = None,
) -> str:
    """
    Generate a markdown release report for a single run.

    Args:
        run_dir: Path to the run directory
        config: Run configuration dictionary
        manifest: Run manifest dictionary
        metrics: Optional detailed metrics
        perf: Optional performance data

    Returns:
        Markdown report string
    """
    report = []

    # Header
    run_hash = run_dir.name
    run_name = config.get("run_name", "Unknown")
    timestamp = manifest.get("timestamp", datetime.now().isoformat())

    report.append(f"# Release Report: {run_name}")
    report.append("")
    report.append(f"**Run Hash:** `{run_hash}`")
    report.append(f"**Generated:** {timestamp}")
    report.append("")

    # Safety disclaimer
    report.append(SAFETY_DISCLAIMER)
    report.append("")

    # Configuration section
    report.append("## Configuration")
    report.append("")
    report.append("| Parameter | Value |")
    report.append("|-----------|-------|")

    vfd_config = config.get("vfd", {})
    report.append(f"| Seed | {config.get('seed', 'N/A')} |")
    report.append(f"| Cell Count | {vfd_config.get('cell_count', 'N/A')} |")
    report.append(f"| Internal Dim | {vfd_config.get('internal_dim', 'N/A')} |")
    report.append(f"| Orbit Size | {vfd_config.get('orbit_size', 'N/A')} |")
    report.append(f"| Orbit Count | {vfd_config.get('orbit_count', 'N/A')} |")
    report.append(f"| Propagation Range | {vfd_config.get('local_propagation_L', 'N/A')} |")

    bridge_config = config.get("bridge", {})
    report.append(f"| Bridge Mode | {bridge_config.get('bridge_mode', 'OFF')} |")

    report.append("")

    # Closure Ladder Results
    report.append("## Closure Ladder Results")
    report.append("")

    closure_results = manifest.get("closure_results", {})
    max_passed = closure_results.get("max_level_passed", "None")
    all_passed = closure_results.get("all_passed", False)

    status = "**PASS**" if all_passed else f"**MAX: {max_passed}**"
    report.append(f"Overall Status: {status}")
    report.append("")

    report.append("| Level | Status | Residual | Families |")
    report.append("|-------|--------|----------|----------|")

    residuals_per_level = closure_results.get("residuals_per_level", {})
    for level in ["L0", "L1", "L2", "L3", "L4"]:
        level_data = residuals_per_level.get(level, {})
        if level_data:
            satisfied = level_data.get("satisfied", False)
            status = "PASS" if satisfied else "FAIL"
            residual = level_data.get("total_residual", 0.0)
            families = level_data.get("family_residuals", {})
            family_str = ", ".join(f"{k}: {v:.2e}" for k, v in families.items())
            report.append(f"| {level} | {status} | {residual:.2e} | {family_str} |")

    report.append("")

    # Bridge Results (if applicable)
    bridge_data = manifest.get("bridge", {})
    if bridge_data and bridge_data.get("mode") != "OFF":
        report.append("## Bridge Axiom Results")
        report.append("")

        metrics_data = bridge_data.get("metrics", {})
        overlay = metrics_data.get("overlay", {})

        report.append("| Metric | Value |")
        report.append("|--------|-------|")
        report.append(f"| Mode | {bridge_data.get('mode', 'N/A')} |")
        report.append(f"| Zeros Compared | {bridge_data.get('zeros_compared', 'N/A')} |")
        report.append(f"| Correlation | {overlay.get('correlation', 'N/A'):.6f} |")
        report.append(f"| Rank Correlation | {overlay.get('rank_correlation', 'N/A'):.6f} |")
        report.append(f"| RMSE | {overlay.get('rmse', 'N/A'):.4f} |")
        report.append(f"| Mean Beta | {overlay.get('mean_beta', 'N/A'):.6f} |")

        falsification = bridge_data.get("falsification", {})
        if falsification:
            report.append("")
            report.append("### Falsification Ratios")
            report.append("")
            report.append("| Negation | Ratio | Status |")
            report.append("|----------|-------|--------|")
            for key in ["BN1_ratio", "BN2_ratio", "BN3_ratio"]:
                ratio = falsification.get(key, 0)
                status = "PASS" if ratio > 1.5 else "FAIL"
                report.append(f"| {key} | {ratio:.2f} | {status} |")

            all_worse = falsification.get("all_negations_worse", False)
            report.append("")
            report.append(f"All Negations Worse: **{'YES' if all_worse else 'NO'}**")

        report.append("")

    # Figures section
    report.append("## Generated Figures")
    report.append("")

    figures_dir = run_dir / "figures"
    if figures_dir.exists():
        figures = sorted(figures_dir.glob("*.png"))
        if figures:
            report.append("| Figure | Description |")
            report.append("|--------|-------------|")

            figure_descriptions = {
                "fig01_residual_ladder.png": "Residual ladder showing constraint satisfaction per level",
                "fig03_constraint_waterfall.png": "Waterfall plot of constraint families",
                "fig04_spectrum_histogram.png": "Spectrum histogram and positivity visualization",
                "fig04_positivity_wall.png": "Legacy: Spectrum histogram (pre-rename)",
                "fig05_collapse_geometry.png": "Collapse geometry visualization",
                "fig06_zero_overlay.png": "Bridge projection: VFD vs reference zeros",
                "fig07_falsification.png": "Falsification test comparisons",
            }

            for fig in figures:
                desc = figure_descriptions.get(fig.name, "Custom figure")
                report.append(f"| `{fig.name}` | {desc} |")
        else:
            report.append("No figures generated.")
    else:
        report.append("Figures directory not found.")

    report.append("")

    # Performance section (if available)
    if perf:
        report.append("## Performance")
        report.append("")

        total_time = perf.get("total_time_ms", 0) / 1000
        report.append(f"**Total Time:** {total_time:.2f} seconds")
        report.append("")

        steps = perf.get("steps", {})
        if steps:
            report.append("| Step | Time (ms) |")
            report.append("|------|-----------|")
            for step, data in steps.items():
                if isinstance(data, dict):
                    time_ms = data.get("duration_ms", 0)
                else:
                    time_ms = data
                report.append(f"| {step} | {time_ms:.1f} |")

        report.append("")

    # Reproducibility section
    report.append("## Reproducibility")
    report.append("")

    git_info = get_git_info()
    python_info = get_python_info()

    report.append("### Environment")
    report.append("")
    report.append("| Property | Value |")
    report.append("|----------|-------|")
    report.append(f"| Git Commit | `{git_info['git_hash']}` |")
    report.append(f"| Git Branch | {git_info['git_branch']} |")
    report.append(f"| Uncommitted Changes | {git_info['git_dirty']} |")
    report.append(f"| Python Version | {python_info['python_version']} |")
    report.append(f"| Platform | {python_info['platform']} |")
    if "numpy_version" in python_info:
        report.append(f"| NumPy Version | {python_info['numpy_version']} |")
    if "scipy_version" in python_info:
        report.append(f"| SciPy Version | {python_info['scipy_version']} |")

    report.append("")

    report.append("### Reproduction Command")
    report.append("")
    report.append("```bash")
    cmd = f"rhdiag run --seed {config.get('seed', 42)}"
    cmd += f" --cell-count {vfd_config.get('cell_count', 32)}"
    cmd += f" --internal-dim {vfd_config.get('internal_dim', 12)}"
    cmd += f" --propagation-range {vfd_config.get('local_propagation_L', 3)}"
    if bridge_config.get("bridge_mode", "OFF") != "OFF":
        cmd += f" --bridge-mode {bridge_config.get('bridge_mode')}"
    report.append(cmd)
    report.append("```")

    report.append("")

    # Footer
    report.append("---")
    report.append("")
    report.append(f"*Report generated by VFD Proof Dashboard v1.0*")
    report.append(f"*Timestamp: {datetime.now().isoformat()}*")

    return "\n".join(report)


def write_release_report(run_dir: Path) -> Path:
    """
    Write a release report for a run directory.

    Args:
        run_dir: Path to run directory containing config.json, manifest.json, etc.

    Returns:
        Path to the written report file
    """
    # Load required files
    config_file = run_dir / "config.json"
    manifest_file = run_dir / "manifest.json"
    metrics_file = run_dir / "metrics.json"
    perf_file = run_dir / "perf.json"

    if not config_file.exists():
        raise FileNotFoundError(f"Config not found: {config_file}")
    if not manifest_file.exists():
        raise FileNotFoundError(f"Manifest not found: {manifest_file}")

    config = json.loads(config_file.read_text())
    manifest = json.loads(manifest_file.read_text())

    metrics = None
    if metrics_file.exists():
        try:
            metrics = json.loads(metrics_file.read_text())
        except json.JSONDecodeError:
            pass

    perf = None
    if perf_file.exists():
        try:
            perf = json.loads(perf_file.read_text())
        except json.JSONDecodeError:
            pass

    # Generate report
    report_content = generate_release_report(
        run_dir=run_dir,
        config=config,
        manifest=manifest,
        metrics=metrics,
        perf=perf,
    )

    # Write report
    report_file = run_dir / "RELEASE_REPORT.md"
    report_file.write_text(report_content)

    return report_file
